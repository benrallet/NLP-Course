{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Students:__ benra741, enral465 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Get the webpage content by using functions in \n",
    "__[urllib module](https://docs.python.org/3/library/urllib.html#module-urllib)__.\n",
    "\n",
    "Other libraries are also fine to achieve the crawling.\n",
    "\n",
    "e.g. scrapy, beautifulsoup... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "x = urllib.request.urlopen('https://play.google.com/store/apps/category/GAME?hl=en').read().decode('utf-8')\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Get app url by regular expression using functions from __[re module](https://docs.python.org/3/library/re.html?highlight=re#module-re)__.\n",
    "\n",
    "A useful online regular expression check.\n",
    "__[Check your regular expression first](https://regex101.com)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "appreg = r'href=\\\"(/store/apps/details.*?)\\\"'\n",
    "appre = re.compile(appreg)\n",
    "app_url_list = re.findall(appre,x)\n",
    "app_url_list = list(set(app_url_list))\n",
    "#print(len(app_url_list))\n",
    "#print(app_url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Access specific webpage to get description of each app and then store the description in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "descg = r'itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">(\\w.*?)<\\/div>'\n",
    "desc = re.compile(descg)\n",
    "\n",
    "\n",
    "def gettingDescription(html):\n",
    "    descg = r'itemprop=\\\"description.*?\\\">.*?<div jsname=\\\".*?\\\">(\\w.*?)<\\/div>'\n",
    "    desc = re.compile(descg)\n",
    "    desc_url = re.findall(desc,x)[0]\n",
    "    remove_tags = re.compile('<.*?>')\n",
    "    desc_url = re.sub(remove_tags, '', desc_url)\n",
    "    desc_url = re.sub('https:\\/\\/.*?(\\s|\\')', '', desc_url)\n",
    "    return desc_url.encode('utf8')\n",
    "\n",
    "def getAppTitle(raw_html):\n",
    "    titlePattern = \"itemprop=\\\"name\\\"> <div class=\\\"id-app-title\\\" tabindex=\\\"0\\\">.*?</div>\"\n",
    "    title = re.findall(titlePattern, str(raw_html))\n",
    "    title = cleanhtml(str(title))\n",
    "    return(title)\n",
    "\n",
    "def urlToDesc(url):\n",
    "    descPattern = \"itemprop=\\\"description.?\\\">.?<div jsname=\\\".?\\\">.?</div>\"\n",
    "    rawHTML_temp = urlopen(url).read().decode('utf8')\n",
    "    desc_temp = re.findall(descPattern, str(rawHTML_temp))\n",
    "    desc_temp = cleanhtml(str(desc_temp))\n",
    "    title_temp = getAppTitle(rawHTML_temp)\n",
    "    return title_temp, desc_temp\n",
    "\n",
    "\n",
    "app_url_list\n",
    "    \n",
    "app_url_done = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < 5 :\n",
    "    url = app_url_list[0]\n",
    "    app_url_list = app_url_list[1:]\n",
    "    app_url_done.append(url) # done list to avoid reviews\n",
    "\n",
    "    x = urllib.request.urlopen('https://play.google.com' + url +\"&hl=en\").read().decode('utf8')\n",
    "    nameg = r'<title id=\\\"main-title\\\">(.*?) - Android'\n",
    "    name = re.compile(nameg)\n",
    "    name2 = re.findall(name, x)[0]\n",
    "    #print(name2)\n",
    "    desc_url = gettingDescription(x)\n",
    "    #print(desc_url)\n",
    "    temp = re.findall(appre, x)\n",
    "    temp = list(set(temp))\n",
    "    app_url_list = [url for url in app_url_list + temp if url not in app_url_done]        \n",
    "\n",
    "    #print(app_url_done)\n",
    "    i = i + 1\n",
    "    file_name = 'description_{}.txt'.format(name2)\n",
    "    with open(file_name , 'w') as file:\n",
    "        print(desc_url, file = file)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Inverted file index (Vector Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Preprocess text using NLP techniques from __[nltk module](http://www.nltk.org/py-modindex.html)__.\n",
    "\n",
    "Using nltk.download(ID) to get the corpora if it is not downloaded before. __[nltk corpora](http://www.nltk.org/nltk_data/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EjarWare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#with open('students.tsv') as fin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'welcome', 'world', 'merge', 'farm', 'newest', 'farming', 'merge', 'game', 'gram', 'games', 'merge', 'farm', 'xe2', 'x80', 'x99t', 'average', 'farming', 'game', 'use', 'merging', 'mechanics', 'achieve', 'task', 'grow', 'farm', 'harvest', 'delicious', 'fruits', 'veggies', 'plant', 'tons', 'crops', 'across', 'farm', 'merge', 'crops', 'grow', 'bigger', 'plants', 'produce', 'fruit', 'veggies', 'take', 'massive', 'harvest', 'sell', 'grow', 'farm', 'even', 'waiting', 'let', 'xe2', 'x80', 'x99s', 'start', 'merging', 'farm', 'merge', 'farm', 'features', 'plant', 'amp', 'merge', 'crops', 'xe2', 'x80', 'xa2', 'plant', 'crops', 'varying', 'types', 'field', 'become', 'available', 'xe2', 'x80', 'xa2', 'merge', 'similar', 'crops', 'grow', 'upgrade', 'providing', 'larger', 'harvest', 'xe2', 'x80', 'xa2', 'bigger', 'plants', 'fruit', 'vegetables', 'produce', 'manage', 'farm', 'xe2', 'x80', 'xa2', 'farming', 'endless', 'means', 'xe2', 'x80', 'x99ll', 'need', 'manage', 'farm', 'daily', 'xe2', 'x80', 'xa2', 'choose', 'plant', 'crops', 'place', 'animals', 'xe2', 'x80', 'xa2', 'decorate', 'placing', 'garden', 'gnomes', 'tractors', 'around', 'farm', 'harvest', 'like', 'farmer', 'xe2', 'x80', 'xa2', 'good', 'farmers', 'know', 'takes', 'time', 'crops', 'produce', 'delicious', 'fruit', 'xe2', 'x80', 'xa2', 'harvest', 'crops', 'whenever', 'keep', 'eye', 'orders', 'traveling', 'merchant', 'top', 'screen', 'xe2', 'x80', 'xa2', 'fill', 'orders', 'make', 'money', 'buy', 'crops', 'grow', 'farm', 'think', 'merge', 'harvest', 'crops', 'build', 'farm', 'keep', 'merging', 'crops', 'discover', 'new', 'forms', 'upgrades', 'see', 'great', 'farm', 'grow', 'download', 'merge', 'farm', 'today', 'start', 'farming']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.add('br')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "with open(file_name) as fin:\n",
    "    for line in fin:\n",
    "    #    tokens = [ word for sent in sent_tokenize(line) for word in word_tokenize(sent)]\n",
    "        #tokens = tokenizer.tokenize(line) \n",
    "        tokens = [ token.lower() for token in tokens ]\n",
    "        tokens = [ token for token in tokens if token not in stop]\n",
    "        \n",
    "\n",
    "print(tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...)Compute tdidf using functions from __[scikit-learn module](http://scikit-learn.org/stable/modules/classes.html)__.\n",
    "\n",
    "eg. TfidfVectorizer is used for converting a collection of raw documents to a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.46979139  0.58028582  0.38408524  0.          0.\n",
      "   0.38408524  0.          0.38408524]\n",
      " [ 0.          0.27925389  0.          0.22830836  0.          0.87501037\n",
      "   0.22830836  0.          0.22830836]\n",
      " [ 0.51184851  0.          0.          0.26710379  0.51184851  0.\n",
      "   0.26710379  0.51184851  0.26710379]\n",
      " [ 0.          0.46979139  0.58028582  0.38408524  0.          0.\n",
      "   0.38408524  0.          0.38408524]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transvector = TfidfVectorizer()\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',]\n",
    "tfidf1 = transvector.fit_transform(corpus)\n",
    "print(tfidf1.toarray())\n",
    "print(transvector.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Process\n",
    "\n",
    "eg. \"Dragon, Control, hero, running\"\n",
    "\n",
    "eg. \"The hero controls the dragon to run.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
